---
title: "p8105_hw6_jz3183"
author: "Jungang Zou"
date: "11/14/2019"
output: github_document
---

```{r set up, include = FALSE}
# knitr will run the chunk but not include the chunk in the final document
# copy from Jeff

# ensure reproductivity
set.seed(10)

# load library
library(tidyverse)
library(viridis)
library(ggridges)
library(patchwork)
library(rvest)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
  # display the code in the code truck above its results in the final document
  echo = TRUE,
  # do not display any warning messages generated by the code
  warning = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

# setting a global options for continuous data color family and a different format to set discrete data to have a color family
options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# have a minimal theme and legends at the bottom
theme_set(theme_get() + theme(legend.position = "bottom"))
```

# Homework 6 for Data Science Course p8105

## Problem 1

First, we read the data from the local file.

```{r p1_read_data, message = FALSE, collapse = TRUE}
# in this code chunk, we read the data from local files and display
df_birthweight = 
  read_csv("./problem1_data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  print()

```

After reading the data, I will tidy the data.

For the missing value, there are totally `r sum(is.na(df_birthweight))` missing values in the data.

For variable type, I tidy the data according to the data description:

* babysex: baby’s sex (male = 1, female = 2)
* bhead: baby’s head circumference at birth (centimeters)
* blength: baby’s length at birth (centimeteres)
* bwt: baby’s birth weight (grams)
* delwt: mother’s weight at delivery (pounds)
* fincome: family monthly income (in hundreds, rounded)
* frace: father’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other, 9 = Unknown)
* gaweeks: gestational age in weeks
* malform: presence of malformations that could affect weight (0 = absent, 1 = present)
* menarche: mother’s age at menarche (years)
* mheigth: mother’s height (inches)
* momage: mother’s age at delivery (years)
* mrace: mother’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other)
* parity: number of live births prior to this pregnancy
* pnumlbw: previous number of low birth weight babies
* pnumgsa: number of prior small for gestational age babies
* ppbmi: mother’s pre-pregnancy BMI
* ppwt: mother’s pre-pregnancy weight (pounds)
* smoken: average number of cigarettes smoked per day during pregnancy
* wtgain: mother’s weight gain during pregnancy (pounds)



```{r p1_data_tidy, collapse = TRUE}
# in this code chunk, we will change the variable type.
df_tidy_birthweight =
  df_birthweight %>% 
  mutate(
    babysex = as.factor(case_when(
       babysex == 1 ~ "male",
       babysex == 2 ~ "female",
    )),
    frace = as.factor(case_when(
      frace == 1 ~ "White",
      frace == 2 ~ "Black",
      frace == 3 ~ "Asian",
      frace == 4 ~ "Puerto Rican",
      frace == 8 ~ "Other",
      frace == 9 ~ "Unknown",
    )),
    malform = as.factor(case_when(
      malform == 0 ~ "absent",
      malform == 1 ~ "present"
    )),
    mrace = as.factor(case_when(
      mrace == 1 ~ "White",
      mrace == 2 ~ "Black",
      mrace == 3 ~ "Asian",
      mrace == 4 ~ "Puerto Rican",
      mrace == 8 ~ "Other",
    ))
  )

#display
df_tidy_birthweight
```

After change the variable type, we need to check if there is error points in data, so I check the structure of the tidied data as follows:

```{r p1_data_tidy_structre, echo = FALSE}
# in this code chunk, we will check the structure of data.
df_tidy_birthweight %>% 
  summary()
```

With the table, we find the factor variables are consistent with the description. In addition, we find the values of variables "pnumlbw" and "pnumsga" are 0 for all the observations.

Since we know just little knowledges, so we cannot use a hypothesized structure for the factors that underly birthweight. In statistical learning, there is a variant of linear regression that is "Lasso", which is a data-driven models, and is robust to some "unuseful" variables. Since lasso make a L1-constraint on the coefficients, it can shrink the unstable variables to 0 and make the model robust and easy to interpret.

First, lasso has a parameter $\lambda$ that control the size of shrinkage, and we need to specify this parameter. we first draw a plot about the profiles of lasso coefficients by tuning the parameter $\lambda$, and use mean of square error as the cost function:

```{r p1_lasso_plot, echo = FALSE}
# in this code chunk, we make a line plot about the profiles of lasso coefficients.

# make the independent variables
x_process = function(df_tidy_birthweight){
  x = 
    df_tidy_birthweight %>% 
    select(-bwt) %>% 
    model.matrix( ~ ., .)
  x
}



# make the response variables
y_process = function(df_tidy_birthweight){
  y = 
    df_tidy_birthweight %>% 
    select(bwt) %>% 
    pull(bwt)
  y
}
# here we use mean of square error as the cost function.
lasso_fit = 
  glmnet::glmnet(x = x_process(df_tidy_birthweight), y = y_process(df_tidy_birthweight), alpha = 1) %>%  #alpha is the parameters to control the type of elastic net models, when alpha = 1, the elastic net model is equivalent to lasso.
  broom::tidy(.)


# draw the plot
lasso_fit %>%
  filter(term != "(Intercept)") %>% 
  ggplot(aes(x = lambda, estimate, color = term)) + 
  geom_line() + 
  scale_x_reverse() +
  labs(
    title = "Lineplot of the Lasso Estimated Coefficients for Different Lambda",
    x = "Lambda",
    y = "Estimated Coefficients",
    caption = "Data from URL: https://p8105.com/data/birthweight.csv") +
  theme(plot.title = element_text(hjust = 0.5))

```

From the graph, we may find the estimated coefficients of variabls "bhead", "blength" have the greatest positive impact on the babyweight as lambda changes. And also, the variable "mraceWhite" also has a little negative impact on the babyweight. On the other hand, when lambda is large, other variables are close to 0, which means they are not robust to the regression, and are not as useful as "bhead", "blength". In the later process, we will carefully delete these variables in regression models based on cross validation.

To carefully choose the parameter lambda, we use cross validation to choose this value.

```{r p1_lasso_cv}
# in this code chunk, we use cv to select the parameter of lasso

# calculate the best lambda
lambda_best = 
  glmnet::cv.glmnet(x = x_process(df_tidy_birthweight), y = y_process(df_tidy_birthweight), alpha = 1, nfolds = 100)

# display the result
lambda_best
```

From the result, we choose lambda = `r lambda_best$lambda.min` as the best parameter, and we have successfully built the regression for this data.

```{r p1_lasso_coefficient}
# in this code chunk, we calculate the coefficients of lasso.

lambda = lambda_best$lambda.min

lasso_best_fit = 
  glmnet::glmnet(x = x_process(df_tidy_birthweight), y = y_process(df_tidy_birthweight), alpha = 1, nlambda = 1, lambda = lambda) #alpha is the parameters to control the type of elastic net models, when alpha = 1, the elastic net model is equivalent to lasso.

# display the parameters
glmnet::coef.glmnet(lasso_best_fit) 
```

From this model, we find the variables "fraceBlack", "fraceOther", "fraceWhite", "malformpresent", "mracePuerto Rican", "pnumlbw", "pnumsga", "ppbmi", "ppwt" have shrink to 0, which means these variables have no effect to predict babyweight. 

On the other hand, variables "bhead", "blength", "mraceWhite", "parity" have show strong positive effect on babyweight, which means the larger these continous variables hold, the larger babyweight is. Also, if baby mother is white, the babyweight is more likely to be greater. What`s more, variables "babysexmale", "fracePuerto Rican", "mraceBlack" have show strong negative effect on babyweight, which means if the baby is male or father is Puerto Rican or mother is black, the babyweight is more likely to be lower. Other variables not mentioned show little effect on the babyweight.

After the regression interpretation, we make the prediction to fitted value and residuals. Also, since functions add_predictions and add_residuals are not consistent with Lasso model, I use the function glmnet::predict.glmnet to predict the fitted value.

```{r p1_lasso_prediction, collapse = TRUE}
# in this code chunk, we use lasso to predict

# calculate the prediction
# since functions add_predictions and add_residuals are not consistent with Lasso model, I use the function glmnet::predict.glmnet to predict the fitted value
prediction = glmnet::predict.glmnet(lasso_best_fit, x_process(df_tidy_birthweight), type = "response")

# calculate the residual
df_tidy_lass = 
  df_tidy_birthweight %>% 
  
  mutate(
    lasso_prediction = prediction,
    lasso_residual = pull(., bwt) - lasso_prediction
    )

# residual plot
df_tidy_lass %>% 
  ggplot(aes(x = lasso_prediction, y = lasso_residual, color = bhead)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Residual Plot of Lasso for Lambda = 0.993",
    x = "Fitted Value",
    y = "Residual",
    caption = "Data from URL: https://p8105.com/data/birthweight.csv") +
  geom_hline(aes(yintercept = 0), color = "black") +
  theme(plot.title = element_text(hjust = 0.5))
```

From the residual plot, we find most of the residual are distributed on the two sides of the 0-axis. Also, we find few outliers when fitted value is low with low value of "bhead".

To show the usage of functions add_predictions and add_residuals, I also use the "useful" variables discussed above (babysex, bhead, blength, frace, mrace, parity), to fit a linear regression and plot the residual against fitted values.

```{r p1_lr}
# in this code chunk, I use the the "useful" variables (babysex, bhead, blength, frace, mrace, parity) to make a regression to show the usage of functions add_predictions and add_residuals.

# fit the linear regression
fit = lm(bwt ~ babysex + bhead + blength + frace + mrace+ parity, data = df_tidy_birthweight)

# make the residual plot and use functions add_predictions and add_residuals.
df_tidy_birthweight %>% 
  mutate(
    residual = pull(modelr::add_residuals(., fit), resid),
    fitted_values = pull(modelr::add_predictions(., fit), pred)) %>% 
  ggplot(aes(x = fitted_values, y = residual, color = bhead)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Residual Plot of Linear Regression in Subset Variables",
    x = "Fitted Value",
    y = "Residual",
    caption = "Data from URL: https://p8105.com/data/birthweight.csv") +
  geom_hline(aes(yintercept = 0), color = "black") +
  theme(plot.title = element_text(hjust = 0.5))

```

From the residual plot of linear regression, we may find the plot looks like the residual plot of Lasso, since these 2 models use the same variable to fit the regression.


Then, I will use cross validation to compare the lasso regression with the following 2 models:

* One using length at birth and gestational age as predictors (main effects only)
* One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r p1_cv_compare}
# in this code chunk, we need to use cv to make comparisons.

# fit the lasso regression and return the model
lasso_function = function(data){
  lambda_best = glmnet::cv.glmnet(x = x_process(data), y = y_process(data), alpha = 1, nfolds = 100) #cv to find best lambda
  lambda = lambda_best$lambda.min #best lambda
  lasso_best_fit = glmnet::glmnet(x = x_process(data), y = y_process(data), alpha = 1, nlambda = 1, lambda = lambda) #fit the lasso model
  lasso_best_fit
}

# since functions rmse is not consistent with Lasso model, I write the function to caculate the rmse of lasso
lasso_rmse = function(model, data) 
{
  fitted_y = glmnet::predict.glmnet(model, x_process(data), type = "response") # calculate the fitted value
  residuals = data$bwt - fitted_y     # calculate the residuals
  sqrt(mean(residuals^2, na.rm = TRUE))      # calculate the rmse
}


# create the cv data
cv_df =
  crossv_mc(df_tidy_birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

# calculate rmse of each models by cv
cv_df = 
  cv_df %>% 
  mutate( # create the models
    lasso_mod = map(train, ~lasso_function(.x)),
    main_effect_mod  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    interactions_mod = map(train, ~lm(bwt ~ bhead * blength + bhead * babysex + babysex * blength + bhead * babysex * blength, data = .x))
  ) %>% 
  mutate( # calculate the rmse
    rmse_lasso = map2_dbl(lasso_mod, test, ~lasso_rmse(model = .x, data = .y)),
    rmse_main_effect  = map2_dbl(main_effect_mod, test, ~rmse(model = .x, data = .y)),
    rmse_interactions = map2_dbl(interactions_mod, test, ~rmse(model = .x, data = .y))
  )

# display
cv_df
```

After calculating the root mean of square error, we will make a violin plot to show the distribution of rmse.

```{r p1_violin_plot}
# in this code chunk, we will make a violin plot to show the distribution of rmse

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) +
  geom_violin(trim = FALSE, alpha = 0.5) +
  geom_jitter(shape = 16, position = position_jitter(0.2), alpha = 0.15) + #add jitters
  geom_boxplot(width = 0.1, lwd = 0.7) + #add the boxplot
  labs(
    title = "Violin Plot of RMSE in Different Models",
    x = "Models",
    y = "Root Mean of Square Error",
    caption = "Data from URL: https://p8105.com/data/birthweight.csv") +
  theme(plot.title = element_text(hjust = 0.5))
```

From the violin plot, we can find the model using length at birth and gestational age as predictors (main effects only) has the highest rmse, which means it doesn`t fit well for test set. And the model using head circumference, length, sex, and all interactions has the second highest rmse which is totally lower than that in model main effects only. Finally, our model Lasso has the lowest rmse, which means it fitted the test set better than other 2 models.


